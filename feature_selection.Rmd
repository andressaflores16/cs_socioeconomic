---
title: "Feature Selection"
author: "Andressa Flores Salvatierra"
date: "2024-10-09"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999) # disable scientific notation
source("./packages.R") # install & load packages
devtools::install_github("silkeszy/Pomona")
library("Pomona")
```

## Feature selection with Boruta

In the previous section, we did basic data cleaning and manipulation with the objective of merging our two main datasets: the World Bank World Development Indicators (WDI) and the WHO caesarean section rates dataset, compiled from various population surveys and government databases. Our resulting dataset was saved in csv format. Let's load it once more:

```{r, load-data}
# Load your data
data <- read_csv('/Users/andressaflores/Documents/projects/cs_socioeconomic/dados/cs_wdi.csv')
dim(data)
```

We can see from the output that 11 columns have been determined as col_logical(), which means that they are empty (only false values). Although the Random Forest algorithm accepts NA values in its input dataset, the feature selection algorithm only accepts them without NA values, so we will most likely have to impute remaining missing values. First, let's check how many NA values there are in our columns:

```{r, check-empty-columns}
# missing percentage per column
na_check <- sapply(data, function(x) sum(is.na(x)) / length(x) * 100)

# a long format dataframe to check percentages
na_long <- data.frame(
  column = names(na_check),
  na_percentage = round(as.numeric(na_check), 2)
)

print(count(na_long$na_percentage > 80))
```

Although we previously removed empty columns and rows in the initial steps of data cleaning, there are still 212 columns which have over 80% NA. This threshold was chosen for removal, as variables with only 20% of observations will not be very informative in future steps. We will also remove some identifier columns which do not need to be input into the feature selection algorithm and transform the categorical data types:

```{r, remove-empty}
# Convert data.table to data frame
cs_wdi <- as.data.frame(cs_wdi)

cs_wdi_cleaned_all_cols <- cs_wdi %>%
  remove_empty(which = "cols", cutoff = 0.2, quiet = FALSE) %>%
  remove_empty(which = "rows", cutoff = 0.2, quiet = FALSE)

# Define columns to remove
columns_to_remove <- c("country_code", "country", "weight")

# Remove specified columns and select target column
cs_wdi_cleaned <- cs_wdi_cleaned %>%
  select(-all_of(columns_to_remove))

# Convert character columns to factors
factor_cols <- c("region", "income_group")
cs_wdi_cleaned[factor_cols] <- lapply(cs_wdi_cleaned[factor_cols], as.factor)
```

The remaining NA values after this cleaning step will be filled in by an imputation algorithm. We have also chosen a Random Forest based algorithm for the imputation, as "it can be used to impute continuous and/or categorical data including complex interactions and non-linear relations."

```{r, near-zero-variables}
nzv <- nearZeroVar(cs_wdi_cleaned, saveMetrics = TRUE)
print(nzv[nzv$zeroVar | nzv$nzv, ])
```

We'll remove these 4 problematic variables and try to run missForest again:

```{r, remove-problematic-near-zero}
#columns_to_remove <- c("SH.HIV.1524.FE.ZS", "SN.ITK.DEFC.ZS", "TM.TAX.TCOM.BC.ZS")
columns_to_remove <- c("income_group")

cs_wdi_cleaned <- cs_wdi_cleaned %>%
  select(-all_of(columns_to_remove))
```

```{r, imputation-1}
imputed_cs_wdi <- missForest(cs_wdi_cleaned, maxiter = 10, ntree = 100, variablewise = TRUE,
                       decreasing = FALSE, verbose = TRUE,
                       mtry = floor(sqrt(ncol(cs_wdi_cleaned))), replace = TRUE,
                       classwt = NULL, cutoff = NULL, strata = NULL,
                       sampsize = NULL, nodesize = NULL, maxnodes = NULL,
                       xtrue = NA, parallelize = 'no')
```

The output for missForest() is a list. The first item is the actual imputed dataframe and the second is the OOB imputation error estimate for each variable that was imputed. Values close to 0 mean a low error rate and 1 means a high error rate (CHECK). Variables with values close to 1 will probaly be removed. Let's check our outputs first and save them in files:

```{r, imputation-results-df}
imp_cs_wdi <- imputed_cs_wdi$ximp
write_csv(imp_cs_wdi, "/Users/andressaflores/Documents/projects/cs_socioeconomic/dados/imputed_cs_wdi_vars.csv")
saveRDS(imputed_cs_wdi, file = "missForest_output.rds")
head(imp_cs_wdi)
tail(imp_cs_wdi)
```

At first glance, our imputation seems to have worked well. Let's now check the OOB error values to see if there are any variables that might need to be removed:

```{r, imputation-results-oob-1}
imp_oob_error <- imputed_cs_wdi$OOBerror
print(imp_oob_error)
```

Let's save our OOB vector to a dataframe:

```{r, imputation-results-oob-2}
df_oob_error <- data.frame(as.list(imp_oob_error))
print(df_oob_error)
```

According to the reference material regarding missForest, the NRMSE can vary between 0 and 1, with 0 being the best imputation possible and 1 being the worst. Since we have MSE [MSE measures the average squared difference between the imputed values and the actual values (for the non-missing data used to train the model)] as output, we have to convert to NRMSE, which isn't scale-dependent and allows for comparison between variables. Let's transform our values to check the quality of the imputation:

```{r, convert-oob-mse}
# Function to calculate NRMSE
NRMSE <- function(mse, actual_values) {
  sqrt(mse) / (max(actual_values, na.rm = TRUE) - min(actual_values, na.rm = TRUE))
}

# Initialize results vector
results <- vector("list", length(imp_oob_error))
names(results) <- names(imp_cs_wdi)

# Process each variable
for (i in seq_along(imp_oob_error)) {
  var_name <- names(imp_cs_wdi)[i]
  
  if (i == 1) {  # First variable is categorical
    results[[i]] <- list(
      type = "categorical",
      PFC = imp_oob_error[i],  # For categorical, MSE is actually PFC
      interpretation = ifelse(imp_oob_error[i] < 0.2, "Good", 
                       ifelse(imp_oob_error[i] < 0.4, "Moderate", "Poor"))
    )
  } else {  # Remaining variables are numeric
    nrmse <- NRMSE(imp_oob_error[i], imp_cs_wdi[[var_name]])
    results[[i]] <- list(
      type = "numeric",
      MSE = imp_oob_error[i],
      NRMSE = nrmse,
      interpretation = ifelse(nrmse < 0.3, "Good", 
                       ifelse(nrmse < 0.5, "Moderate", "Poor"))
    )
  }
}

# Overall summary
cat("\nOverall Summary:\n")
cat("Good imputations:", sum(sapply(results, function(x) x$interpretation == "Good")), "\n")
cat("Moderate imputations:", sum(sapply(results, function(x) x$interpretation == "Moderate")), "\n")
cat("Poor imputations:", sum(sapply(results, function(x) x$interpretation == "Poor")), "\n")

# Identify variables with poor imputation
poor_vars <- names(results)[sapply(results, function(x) x$interpretation == "Poor")]
if (length(poor_vars) > 0) {
  cat("\nVariables with poor imputation:\n")
  print(poor_vars)
}
```

All of our converted values of NRMSE are close to 0, meaning that all of our variables had a good quality imputation. This means we can go on to the next phase: running the feature selection algorithm to see which variables will be included in our final model. According to *Applying random forest in a health administrative data context: a conceptual guide* and *Evaluation of variable selection methods for random forests and omics data sets* the best performing feature selection algorithm for our type of dataset is **Boruta** (Feature Selection with the Boruta Package, Miron B. Kursa & Witold R. Rudnicki). Let's try and run the alogorithm with the default parameters:

```{r, boruta-feature-selection}
# Identify the target variable column
target_col <- which(names(imp_cs_wdi) == "cs_section_rate")

# Run Boruta
set.seed(123)  # for reproducibility
boruta_output <- Boruta(x = imp_cs_wdi[, -target_col], 
                        y = imp_cs_wdi[, target_col], 
                        doTrace = 2,  # 2 for detailed output, 0 for no output
                        maxRuns = 500)  # increase if Boruta doesn't converge
```

```{r, boruta-output-info}
print(boruta_output)
saveRDS(boruta_output, file = "boruta_output.rds")
```

```{r, boruta-indicator-stats}
# Get more detailed information about the indicators
boruta_results <- attStats(boruta_output)
print(boruta_results)
```


```{r, select-confirmed-indicators}
# sort indicators by importance and select only confirmed ones
confirmed_indicators <- boruta_results %>%
  tibble::rownames_to_column(var = "indicator_code") %>%
  select(indicator_code, meanImp, decision) %>%
  arrange(desc(meanImp)) %>%
  filter(decision == 'Confirmed')

print(confirmed_indicators)
```

We have 1012 confirmed important predictors. The Boruta is an all inclusive algorithm, so it will confirm all predictors it detects, without trying to reduce to a minimal ideal, more as an exploratory analysis. This means that we need to consult the reference literature and try to identify evidence, from previous works, as to which variables are more relevant for an accurate prediction model.
In the future we can test different parameters or other feature selection algorithms. 

For now, let's understand our indicators and which topics they are related to.

### Selecting variables with domain knowledge

We have some metadata regarding our socioeconomic indicators. Let's filter our metadata based on the indicators selected by Boruta:

```{r, load-indicator-metadata}
indicator_metadata <- read_csv("dados/WDI_CSV/WDISeries.csv", col_select = c(1, 2, 3, 5, 6), col_types = cols(
  "Series Code" = col_character(),
  "Topic" = col_character(),
  "Indicator Name" = col_character(),
  "Long definition" = col_character(),
  "Unit of measure" = col_character()
))

indicator_metadata <- indicator_metadata %>%
  clean_names() %>%
  dplyr::rename(indicator_code = 1) %>%
  relocate(indicator_name, .before = topic)

glimpse(indicator_metadata)
```

```{r, more-indicator-info}
selected_indicators_metadata <- confirmed_indicators %>%
  select(-decision) %>%
  left_join(indicator_metadata, by = c("indicator_code" = "indicator_code"))
print(selected_indicators_metadata)
```

We have decided to use the first 20 indicators from the confirmed indicators to see if we can get a precise model. Let's return to the input dataset from Boruta:

```{r, input-rf-dataset-1}
dim(imp_cs_wdi)
head(colnames(imp_cs_wdi), 20)
tail(colnames(imp_cs_wdi), 20)
```
Let's rejoin important identifier columns that we removed before the imputation process:

```{r, input-rf-dataset-2}
# cs_wdi_cleaned_all_cols is the df one step before the Boruta input dataset
columns_to_move <- cs_wdi_cleaned_all_cols %>% select(country_code, country, income_group, weight)

# adding back removed columns
imp_cs_wdi_complete <- imp_cs_wdi %>%
  add_column(columns_to_move, .before = 1) %>%
  relocate(weight, .before = cs_section_rate)

# naming rows + row number with unique identifier
rownames(imp_cs_wdi_complete) <- paste(seq_len(nrow(imp_cs_wdi_complete)), paste(imp_cs_wdi_complete$country_code, imp_cs_wdi_complete$year, sep = "_"), sep = "_")

# previewing dataset
dim(imp_cs_wdi_complete)
head(colnames(imp_cs_wdi_complete), 20)
tail(colnames(imp_cs_wdi_complete), 20)
```
Now we can use the top 20 indicators from the Boruta output and filter our dataset:

```{r, input-rf-dataset-3}
# selecting indicators that will be used in the model
top_20_indicators <- confirmed_indicators %>%
  slice_head(n = 20) %>%
  select(indicator_code) %>%
  unlist(use.names = FALSE)

#columns that are still 'chr' 
columns_to_factor <- c("country_code", "country", "income_group")

# select top 20 indicator columns from 1152 select by Boruta + converting cols to factor
imp_cs_wdi_top20 <- imp_cs_wdi_complete %>%
  select(1:5, (last_col()-1):last_col(), all_of(top_20_indicators)) %>%
  mutate(
    country_code = as.factor(country_code),
    country = as.factor(country),
    income_group = as.factor(income_group)
  )

str(imp_cs_wdi_top20)
```














