---
title: "Feature Selection"
author: "Andressa Flores Salvatierra"
date: "2024-10-09"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999) # disable scientific notation
source("./packages.R") # install & load packages
devtools::install_github("silkeszy/Pomona")
library("Pomona")
```

## Feature selection with Boruta

In the previous section, we did basic data cleaning and manipulation with the objective of merging our two main datasets: the World Bank World Development Indicators (WDI) and the WHO caesarean section rates dataset, compiled from various population surveys and government databases. Our resulting dataset was saved in csv format. Let's load it once more:

```{r, load-data}
# Load your data
data <- read_csv('/Users/andressaflores/Documents/projects/cs_socioeconomic/dados/cs_wdi.csv')
dim(data)
```
We can see from the output that 11 columns have been determined as col_logical(), which means that they are empty (only false values). Although the Random Forest algorithm accepts NA values in its input dataset, the feature selection algorithm only accepts them without NA values, so we will most likely have to impute remaining missing values. First, let's check how many NA values there are in our columns:

```{r, check-empty-columns}
# missing percentage per column
na_check <- sapply(data, function(x) sum(is.na(x)) / length(x) * 100)

# a long format dataframe to check percentages
na_long <- data.frame(
  column = names(na_check),
  na_percentage = round(as.numeric(na_check), 2)
)

print(count(na_long$na_percentage > 80))
```
Although we previously removed empty columns and rows in the initial steps of data cleaning, there are still 212 columns which have over 80% NA. This threshold was chosen for removal, as variables with only 20% of observations will not be very informative in future steps. We will also remove some identifier columns which do not need to be input into the feature selection algorithm and transform the categorical data types:

```{r, remove-empty}
# Convert data.table to data frame
cs_wdi <- as.data.frame(cs_wdi)

cs_wdi_cleaned <- cs_wdi %>%
  remove_empty(which = "cols", cutoff = 0.2, quiet = FALSE) %>%
  remove_empty(which = "rows", cutoff = 0.2, quiet = FALSE)

# Define columns to remove
columns_to_remove <- c("country_code", "country", "weight")  # Replace with your column names

# Remove specified columns and select target column
cs_wdi_cleaned <- cs_wdi_cleaned %>%
  select(-all_of(columns_to_remove))

# Convert character columns to factors
factor_cols <- c("region", "income_group")
cs_wdi_cleaned[factor_cols] <- lapply(cs_wdi_cleaned[factor_cols], as.factor)
```

The remaining NA values after this cleaning step will be filled in by an imputation algorithm. We have also chosen a Random Forest based algorithm for the imputation, as "it can be used to impute continuous and/or categorical data including complex interactions and non-linear relations."

```{r}
nzv <- nearZeroVar(cs_wdi_cleaned, saveMetrics = TRUE)
print(nzv[nzv$zeroVar | nzv$nzv, ])
```

We'll remove these 4 problematic variables and try to run missForest again:

```{r}
columns_to_remove <- c("SH.HIV.1524.FE.ZS", "SN.ITK.DEFC.ZS", "TM.TAX.TCOM.BC.ZS")

cs_wdi_cleaned <- cs_wdi_cleaned %>%
  select(-all_of(columns_to_remove))
```


```{r}
summary_data <- data.frame(
  Variable = names(cs_wdi_cleaned),
  Class = sapply(cs_wdi_cleaned, class),
  NAs = sapply(cs_wdi_cleaned, function(x) sum(is.na(x))),
  Unique = sapply(cs_wdi_cleaned, function(x) length(unique(x))),
  stringsAsFactors = FALSE
)
print(summary_data)
```


```{r eval=FALSE, include=FALSE}
# Assuming the last column is your target variable
target_column <- names(data_clean)[ncol(data_clean)]

# Run Boruta
set.seed(123)  # for reproducibility
boruta_output <- Boruta(formula = reformulate(".", response = target_column), 
                        data = data_clean, 
                        doTrace = 2,  # Set to 2 for more verbose output
                        maxRuns = 500)  # Adjust as needed
```

```{r eval=FALSE, include=FALSE}
# Print Boruta results
print(boruta_output)

# Plot Boruta results
plot(boruta_output)

# Get confirmed important variables
important_vars <- getSelectedAttributes(boruta_output, withTentative = FALSE)

# Print important variables
cat("Confirmed important variables:\n")
print(important_vars)

# Handle tentative attributes
boruta_fixed <- TentativeRoughFix(boruta_output)

# Get final list of important variables (including tentative)
final_vars <- getSelectedAttributes(boruta_fixed, withTentative = TRUE)

# Print final variables
cat("\nFinal list of important variables (including tentative):\n")
print(final_vars)

# Create a tibble of attribute decisions
attr_decisions <- attStats(boruta_fixed) %>%
  as_tibble(rownames = "attribute") %>%
  arrange(desc(meanImp))

# Print attribute decisions
print(attr_decisions)

# Optionally, save results
write_csv(attr_decisions, "boruta_results.csv")
```

```{r, imputation-1}
imputed_cs_wdi <- missForest(cs_wdi_cleaned, maxiter = 10, ntree = 100, variablewise = TRUE,
                       decreasing = FALSE, verbose = TRUE,
                       mtry = floor(sqrt(ncol(cs_wdi_cleaned))), replace = TRUE,
                       classwt = NULL, cutoff = NULL, strata = NULL,
                       sampsize = NULL, nodesize = NULL, maxnodes = NULL,
                       xtrue = NA, parallelize = 'no')
```



