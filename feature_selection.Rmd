---
title: "Feature Selection"
author: "Andressa Flores Salvatierra"
date: "2024-10-09"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999) # disable scientific notation
source("./packages.R") # install & load packages
devtools::install_github("silkeszy/Pomona")
library("Pomona")
```

## Feature selection with Boruta

In the previous section, we did basic data cleaning and manipulation with the objective of merging our two main datasets: the World Bank World Development Indicators (WDI) and the WHO caesarean section rates dataset, compiled from various population surveys and government databases. Our resulting dataset was saved in csv format. Let's load it once more:

```{r, load-data}
# Load your data
data <- read_csv('/Users/andressaflores/Documents/projects/cs_socioeconomic/dados/cs_wdi.csv')
dim(data)
```

We can see from the output that 11 columns have been determined as col_logical(), which means that they are empty (only false values). Although the Random Forest algorithm accepts NA values in its input dataset, the feature selection algorithm only accepts them without NA values, so we will most likely have to impute remaining missing values. First, let's check how many NA values there are in our columns:

```{r, check-empty-columns}
# missing percentage per column
na_check <- sapply(data, function(x) sum(is.na(x)) / length(x) * 100)

# a long format dataframe to check percentages
na_long <- data.frame(
  column = names(na_check),
  na_percentage = round(as.numeric(na_check), 2)
)

print(count(na_long$na_percentage > 80))
```

Although we previously removed empty columns and rows in the initial steps of data cleaning, there are still 212 columns which have over 80% NA. This threshold was chosen for removal, as variables with only 20% of observations will not be very informative in future steps. We will also remove some identifier columns which do not need to be input into the feature selection algorithm and transform the categorical data types:

```{r, remove-empty}
# Convert data.table to data frame
cs_wdi <- as.data.frame(cs_wdi)

cs_wdi_cleaned <- cs_wdi %>%
  remove_empty(which = "cols", cutoff = 0.2, quiet = FALSE) %>%
  remove_empty(which = "rows", cutoff = 0.2, quiet = FALSE)

# Define columns to remove
columns_to_remove <- c("country_code", "country", "weight")  # Replace with your column names

# Remove specified columns and select target column
cs_wdi_cleaned <- cs_wdi_cleaned %>%
  select(-all_of(columns_to_remove))

# Convert character columns to factors
factor_cols <- c("region", "income_group")
cs_wdi_cleaned[factor_cols] <- lapply(cs_wdi_cleaned[factor_cols], as.factor)
```

The remaining NA values after this cleaning step will be filled in by an imputation algorithm. We have also chosen a Random Forest based algorithm for the imputation, as "it can be used to impute continuous and/or categorical data including complex interactions and non-linear relations."

```{r}
nzv <- nearZeroVar(cs_wdi_cleaned, saveMetrics = TRUE)
print(nzv[nzv$zeroVar | nzv$nzv, ])
```

We'll remove these 4 problematic variables and try to run missForest again:

```{r}
#columns_to_remove <- c("SH.HIV.1524.FE.ZS", "SN.ITK.DEFC.ZS", "TM.TAX.TCOM.BC.ZS")
columns_to_remove <- c("income_group")

cs_wdi_cleaned <- cs_wdi_cleaned %>%
  select(-all_of(columns_to_remove))
```

```{r}
summary_data <- data.frame(
  Variable = names(cs_wdi_cleaned),
  Class = sapply(cs_wdi_cleaned, class),
  NAs = sapply(cs_wdi_cleaned, function(x) sum(is.na(x))),
  Unique = sapply(cs_wdi_cleaned, function(x) length(unique(x))),
  stringsAsFactors = FALSE
)
print(summary_data)
```

```{r eval=FALSE, include=FALSE}
# Assuming the last column is your target variable
target_column <- names(data_clean)[ncol(data_clean)]

# Run Boruta
set.seed(123)  # for reproducibility
boruta_output <- Boruta(formula = reformulate(".", response = target_column), 
                        data = data_clean, 
                        doTrace = 2,  # Set to 2 for more verbose output
                        maxRuns = 500)  # Adjust as needed
```

```{r eval=FALSE, include=FALSE}
# Print Boruta results
print(boruta_output)

# Plot Boruta results
plot(boruta_output)

# Get confirmed important variables
important_vars <- getSelectedAttributes(boruta_output, withTentative = FALSE)

# Print important variables
cat("Confirmed important variables:\n")
print(important_vars)

# Handle tentative attributes
boruta_fixed <- TentativeRoughFix(boruta_output)

# Get final list of important variables (including tentative)
final_vars <- getSelectedAttributes(boruta_fixed, withTentative = TRUE)

# Print final variables
cat("\nFinal list of important variables (including tentative):\n")
print(final_vars)

# Create a tibble of attribute decisions
attr_decisions <- attStats(boruta_fixed) %>%
  as_tibble(rownames = "attribute") %>%
  arrange(desc(meanImp))

# Print attribute decisions
print(attr_decisions)

# Optionally, save results
write_csv(attr_decisions, "boruta_results.csv")
```

```{r, imputation-1}
imputed_cs_wdi <- missForest(cs_wdi_cleaned, maxiter = 10, ntree = 100, variablewise = TRUE,
                       decreasing = FALSE, verbose = TRUE,
                       mtry = floor(sqrt(ncol(cs_wdi_cleaned))), replace = TRUE,
                       classwt = NULL, cutoff = NULL, strata = NULL,
                       sampsize = NULL, nodesize = NULL, maxnodes = NULL,
                       xtrue = NA, parallelize = 'no')
```

The output for missForest() is a list. The first item is the actual imputed dataframe and the second is the OOB imputation error estimate for each variable that was imputed. Values close to 0 mean a low error rate and 1 means a high error rate (CHECK). Variables with values close to 1 will probaly be removed. Let's check our outputs first and save our imputed dataset to a csv:

```{r, imputation-results-df}
imp_cs_wdi <- imputed_cs_wdi$ximp
write_csv(imp_cs_wdi, "/Users/andressaflores/Documents/projects/cs_socioeconomic/dados/imputed_cs_wdi_vars.csv")
head(imp_cs_wdi)
tail(imp_cs_wdi)
```

At first glance, our imputation seems to have worked well. Let's now check the OOB error values to see if there are any variables that might need to be removed:

```{r, imputation-results-oob-1}
imp_oob_error <- imputed_cs_wdi$OOBerror
print(imp_oob_error)
```

Let's save our OOB vector to a dataframe:

```{r, imputation-results-oob-2}
df_oob_error <- data.frame(as.list(imp_oob_error))
print(df_oob_error)
```

According to the reference material regarding missForest, the NRMSE can vary between 0 and 1, with 0 being the best imputation possible and 1 being the worst. Since we have MSE [MSE measures the average squared difference between the imputed values and the actual values (for the non-missing data used to train the model)] as output, we have to convert to NRMSE, which isn't scale-dependent and allows for comparison between variables. Let's transform our values to check the quality of the imputation:

```{r, convert-oob-mse}
# Function to calculate NRMSE
NRMSE <- function(mse, actual_values) {
  sqrt(mse) / (max(actual_values, na.rm = TRUE) - min(actual_values, na.rm = TRUE))
}

# Initialize results vector
results <- vector("list", length(imp_oob_error))
names(results) <- names(imp_cs_wdi)

# Process each variable
for (i in seq_along(imp_oob_error)) {
  var_name <- names(imp_cs_wdi)[i]
  
  if (i == 1) {  # First variable is categorical
    results[[i]] <- list(
      type = "categorical",
      PFC = imp_oob_error[i],  # For categorical, MSE is actually PFC
      interpretation = ifelse(imp_oob_error[i] < 0.2, "Good", 
                       ifelse(imp_oob_error[i] < 0.4, "Moderate", "Poor"))
    )
  } else {  # Remaining variables are numeric
    nrmse <- NRMSE(imp_oob_error[i], imp_cs_wdi[[var_name]])
    results[[i]] <- list(
      type = "numeric",
      MSE = imp_oob_error[i],
      NRMSE = nrmse,
      interpretation = ifelse(nrmse < 0.3, "Good", 
                       ifelse(nrmse < 0.5, "Moderate", "Poor"))
    )
  }
}

# Print summary
#for (var_name in names(results)) {
#  cat("\nVariable:", var_name, "\n")
#  if (results[[var_name]]$type == "categorical") {
#    cat("Type: Categorical\n")
#    cat("PFC:", results[[var_name]]$PFC, "\n")
#  } else {
#    cat("Type: Numeric\n")
#    cat("MSE:", results[[var_name]]$MSE, "\n")
#    cat("NRMSE:", results[[var_name]]$NRMSE, "\n")
#  }
#  cat("Interpretation:", results[[var_name]]$interpretation, "\n")
#}

# Overall summary
cat("\nOverall Summary:\n")
cat("Good imputations:", sum(sapply(results, function(x) x$interpretation == "Good")), "\n")
cat("Moderate imputations:", sum(sapply(results, function(x) x$interpretation == "Moderate")), "\n")
cat("Poor imputations:", sum(sapply(results, function(x) x$interpretation == "Poor")), "\n")

# Identify variables with poor imputation
poor_vars <- names(results)[sapply(results, function(x) x$interpretation == "Poor")]
if (length(poor_vars) > 0) {
  cat("\nVariables with poor imputation:\n")
  print(poor_vars)
}
```

All of our converted values of NRMSE are close to 0, meaning that all of our variables had a good quality imputation. This means we can go on to the next phase: running the feature selection algorithm to see which variables will be included in our final model. According to *Applying random forest in a health administrative data context: a conceptual guide* and *Evaluation of variable selection methods for random forests and omics data sets* the best performing feature selection algorithm for our type of dataset is **Boruta** (Feature Selection with the Boruta Package, Miron B. Kursa & Witold R. Rudnicki). Let's try and run the alogorithm with the default parameters:

```{r, boruta-feature-selection}
# Identify the target variable column
target_col <- which(names(imp_cs_wdi) == "cs_section_rate")

# Run Boruta
set.seed(123)  # for reproducibility
boruta_output <- Boruta(x = imp_cs_wdi[, -target_col], 
                        y = imp_cs_wdi[, target_col], 
                        doTrace = 2,  # 2 for detailed output, 0 for no output
                        maxRuns = 500)  # increase if Boruta doesn't converge
```

